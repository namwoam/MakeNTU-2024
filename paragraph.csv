,coords,content
0,"[[137, 199], [357, 199], [357, 235], [137, 235]]",1. Introduction
1,"[[995, 221], [1017, 221], [1017, 235], [995, 235]]",Loc
2,"[[1069, 221], [1117, 221], [1117, 235], [1069, 235]]",FaceDet
3,"[[1255, 221], [1293, 221], [1293, 235], [1255, 235]]",Select
4,"[[1347, 221], [1391, 221], [1391, 235], [1347, 235]]",Classify
5,"[[1445, 221], [1469, 221], [1469, 235], [1445, 235]]",Vqa
6,"[[865, 222], [955, 222], [955, 257], [865, 257]]",ImaBe Understanding
7,"[[981, 243], [1029, 243], [1029, 255], [981, 255]]",Oll -ViT
8,"[[1057, 243], [1127, 243], [1127, 257], [1057, 257]]",DSFD (Oypi)
9,"[[1150, 244], [1212, 244], [1212, 252], [1150, 252]]",1751FNNR |
10,"[[1247, 243], [1301, 243], [1301, 255], [1247, 255]]",CLIP-ViT
11,"[[1343, 243], [1395, 243], [1395, 255], [1343, 255]]",CLIP-ViT
12,"[[1443, 243], [1471, 243], [1471, 255], [1443, 255]]",VfLT
13,"[[131, 255], [801, 255], [801, 1990], [131, 1990]]","The pursuit of general purpose AI systems has lead to the development of capable end-to-end trainable models [1,5,8,13,19,25,27], many Of which aspire to provide simple natural language interface for a user to interact with the model。 The predominant approach to building these systems has been massive-scale unsupervised pretraining followed by supervised multitask training: However; this approach requires a well curated dataset for each task that makes i challenging to scale to the infinitely long tail Of complex tasks We would eventually like these systems to perform. I this work, we explore the use oflarge language models to tackle the long tail of complex tasks by decom- posing these tasks described i natural language into sim- pler steps that may be handled by specialized end-to-end trained models or other programs。 Imagine instructing 8 Vision system tO Tag the 7 main characters on the TV show Big Bang Theory in this image. To perform this task。 the system first needs to understand the intent Of the instruction and then perform sequence Of steps detect the faces, retrieve list Of main characters OI Big Bang Theory from knowledge base, classify faces using the list Of characters, and tag the image with recog- nized character s faces and names。 While different Vision and language systems exist to perform each of these steps, executing this task described in natural language is beyond the scope Of end-to-end trained systems。 We introduce VISPROG Which inputs visual data (a sin- gle image Or a set of images) along with a natural language instruction, generates sequence Of steps, visual pro- gram 让 you will, and then executes these steps tO produce the desired output。 Each line 证 a Visual Program invokes OnC among a Wide range Of modules currently supported by the system。 Modules may be Off-the-shelf computer vi- sion models, language models, image processing subrou- tines i OpenCV [4],or arithmetic and logical operators Modules consume inputs that are produced by executing Previous lines of code and output intermediate results that Can be consumed downstream. In the example above, the visual program generated by VISPROG invokes a face de- tector 18],GPT-3 [5] as a knowledge retrieval system, and CLIP [23] as an open-vocabulary image classifier to Pro- duce the desired output (see Fig. 1. VISPROG improves upon previous methods for gener ating and executing programs for vision applications。 For the Visual question answering (VQA) task; Neural Module Networks (NMN) [2,9,10,12] compose question-specific, Cnd-to-end trainable network from specialized; differen- tiable Ieural modules。 These approaches either use brittle, Off-the-shelf semantic parsers to deterministically compute the layout Of modules, or leam layout generator through Weak anSWeI supervision via REINFORCE [33] In COI- trast, VISPROG uses powerful language model (GPT-3)"
14,"[[987, 297], [1033, 297], [1033, 311], [987, 311]]",Replace
15,"[[1065, 294], [1149, 294], [1149, 347], [1065, 347]]","Colorpop PIL ,convert() CVZ SrabCut()"
16,"[[1207, 295], [1247, 295], [1247, 311], [1207, 311]]",BBBlur
17,"[[1437, 295], [1471, 295], [1471, 311], [1437, 311]]",Emoji
18,"[[990, 320], [1028, 320], [1028, 328], [990, 328]]",St3h1e
19,"[[1169, 319], [1281, 319], [1281, 347], [1169, 347]]",PIL .GaU5sianllur( ) Cv2 BrabCut()
20,"[[1301, 321], [1395, 321], [1395, 347], [1301, 347]]",PIL .rectangle() tett()
21,"[[1415, 321], [1491, 321], [1491, 335], [1415, 335]]",AUBLY (Dypi)
22,"[[889, 337], [929, 337], [929, 351], [889, 351]]",Image
23,"[[981, 331], [1041, 331], [1041, 345], [981, 345]]",Diffusion
24,"[[869, 353], [951, 353], [951, 367], [869, 367]]",Manipulation
25,"[[1003, 367], [1031, 367], [1031, 381], [1003, 381]]",Crop
26,"[[1089, 367], [1153, 367], [1153, 401], [1089, 401]]","Cropleft PIL,crop()"
27,"[[1201, 367], [1267, 367], [1267, 401], [1201, 401]]",CropRiBht PIL.crop()
28,"[[1315, 367], [1379, 367], [1379, 401], [1315, 401]]",CrOpAbOVe PIL.crop()
29,"[[1421, 367], [1485, 367], [1485, 401], [1421, 401]]",CropBelow PILcrop()
30,"[[983, 387], [1047, 387], [1047, 401], [983, 401]]","PIL,crop()"
31,"[[999, 441], [1021, 441], [1021, 455], [999, 455]]",Ust
32,"[[1089, 445], [1155, 445], [1155, 479], [1089, 479]]",Arithmetic LoBical
33,"[[1227, 441], [1253, 441], [1253, 455], [1227, 455]]",Eval
34,"[[1329, 441], [1365, 441], [1365, 455], [1329, 455]]",Count
35,"[[1435, 441], [1473, 441], [1473, 455], [1435, 455]]",Result
36,"[[875, 445], [943, 445], [943, 477], [875, 477]]",Knowledge Retrieval
37,"[[995, 463], [1023, 463], [1023, 475], [995, 475]]",GPT
38,"[[1219, 463], [1259, 463], [1259, 477], [1219, 477]]",eval()
39,"[[1329, 463], [1363, 463], [1363, 477], [1329, 477]]",len()
40,"[[1433, 463], [1473, 463], [1473, 477], [1433, 477]]",Jict()
41,"[[849, 516], [1522, 516], [1522, 1984], [849, 1984]]","Figure 2_ Modules currently supported i VISPROG。 Red modules use neural models (OWL-Vir [21], DSFD 18], Mask- Former [6], CLIP [23], ViLT [16], and Stable Diffusion [28]). Blue modules use image processing and other Python subroutines。 These modules are ivoked i programs generated from natural language instructions. Adding new modules to extend VISPROG's capabilities is straightforward (Code.1). and a Small number Of in-context examples to create Com- plex programs without requiring any training' Programs created by VISPROG also use higher-level of abstraction than NMNs and ivoke trained state-of-the-art models and nonl-neural python subroutines (Fig: 2). These advantages make VISPROG 31 easy-to-Use, Performant, and modular neuro-symbolic system。 VISPROG is also highly interpretable。 First。 VISPROG produces easy-to-understand programs Which USEI Can verify for logical correctness_ Second, by breaking down the prediction into simple steps, VISPROG allows 3 USCI t0 inspect the outputs Of intermediate steps to diagnose errors and 让 required, intervene in the reasoning process。 Alto- gether, an executed program with intermediate step results (e.8. text, bounding boxes, segmentation masks, generated images, etc .) linked together to depict the flow ofinforma- tion serves as a visual rationale for the prediction_ To demonstrate ils flexibility, we use VISPROG for 4 dif- ferent tasks that share some common skills (e.8一 for j- age parsing) While also requiring some degree Of special- ized reasoning and visual manipulation capabilities。 These tasks are (i) compositional visual question answering; (i) zero-shot natural language visual reasoning (NLVR) on im- age pairs; (试) factual knowledge object tagging from natu- ral language instructions; and (iv) language-guided image editing: We emphasize that neither the language model nor any Of the modules are finetuned 讥 any way。 Adapt ing VISPROG to any task is as simple as providing a few in-context examples consisting Ofnatural language instruc- tions and the corresponding programs_ While easy tO use VISPROG shows an impressive gain Of 2 points OVCI base VQA model on the compositional VQA task, strong zero-shot accuracy of 62.4% on NLVR without ever train- ig OI image pairs, and delightful qualitative and quantita- tive results on knowledge tagging and image editing tasks。 1We use 'training' to refer to gradient-based learning to differentiate it from in-context learning which only involves a feedforward pass_"
42,"[[1169, 219], [1195, 219], [1195, 237], [1169, 237]]",5e8
43,"[[1335, 295], [1361, 295], [1361, 313], [1335, 313]]",Ta
